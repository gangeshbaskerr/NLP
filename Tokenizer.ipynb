{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a70acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee47e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    " \n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\";\n",
    "text_lowercase(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3983de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  balls in this bag, and  in the other one.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "\tresult = re.sub(r'\\d+', '', text)\n",
    "\treturn result\n",
    "\n",
    "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "remove_numbers(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defcf80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "\ttranslator = str.maketrans('', '', string.punctuation)\n",
    "\treturn text.translate(translator)\n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
    "remove_punctuation(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b12266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\23820\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33f46d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "\tstop_words = set(stopwords.words(\"english\"))\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tfiltered_text = [word for word in word_tokens if word not in stop_words]\n",
    "\treturn filtered_text\n",
    "\n",
    "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
    "remove_stopwords(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fab432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tstems = [stemmer.stem(word) for word in word_tokens]\n",
    "\treturn stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e346b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "def stem_words(text):\n",
    "\tword_tokens = word_tokenize(text)\n",
    "\tstems = [stemmer.stem(word) for word in word_tokens]\n",
    "\treturn stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0a642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi hello how are you dfdfsdf']\n",
      "['hi', 'hello', 'how', 'are', 'you', 'dfdfsdf']\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing \n",
    "# libraries \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "text = \"hi hello how are you dfdfsdf \"\n",
    "print(sent_tokenize(text)) \n",
    "print(word_tokenize(text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1d014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9288098-6b7f-45c1-8a36-20482b5265c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tknobj\u001b[38;5;241m=\u001b[39mWordPunctTokenizer()\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilt in :\u001b[39m\u001b[38;5;124m\"\u001b[39m,tknobj\u001b[38;5;241m.\u001b[39mtokenize(\u001b[43msample\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "tknobj=WordPunctTokenizer()\n",
    "print(\"Built in :\",tknobj.tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd59ec6-5690-4ee5-9a2c-fc6e4433ac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tutorialspoint.com',\n",
       " 'provides',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'technical',\n",
       " 'tutorials',\n",
       " 'for',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "wordori_tokenize('Tutalspoint.com provides high quality technical tutorials for free.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b40cd81b-aba8-438f-9fb1-c67ef69024b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tutorialspoint.com',\n",
       " 'provides',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'technical',\n",
       " 'tutorials',\n",
       " 'for',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer_wrd = TreebankWordTokenizer()\n",
    "tokenizer_wrd.tokenize('Tutorialspoint.com provides high quality technical tutorials for free.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39c83ed-56eb-4b6c-8a94-af637709efaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wo', \"n't\"]\n",
      "['won', \"'\", 't']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"won't\"))\n",
    "print(tknobj.tokenize(\"won't\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a74ca5b-1ac5-4d51-a5d0-16483266e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let us understand the difference between sentence & word tokenizer.',\n",
       " 'It is going to be a simple example.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Let us understand the difference between sentence & word tokenizer. It is going to be a simple example.\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1263d044-540e-497b-a233-06cdcf02d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"won't\", 'is', 'a', 'contraction']\n",
      "[\"can't\", 'is', 'a', 'contraction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\23820\\AppData\\Local\\Temp\\ipykernel_22232\\3163523229.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokenizer = RegexpTokenizer(\"[\\w']+\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "print(tokenizer.tokenize(\"won't is a contraction.\"))\n",
    "print(tokenizer.tokenize(\"can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc81d5d1-8239-40bd-a331-e0521679a9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'writer']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = ['I', 'am', 'a', 'writer']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97da84ab-2d57-4885-b998-4de6f28a5142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6caf4237-3c25-407f-ad1b-a9a14610bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer=nltk.stem.PorterStemmer()\n",
    "word_stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79b7afa0-8607-4ec4-9cc6-0663711b82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(word_stemmer.stem('eating'))\n",
    "print(word_stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0f30fe7-c358-4d8d-bff7-03fb16863444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws=nltk.stem.RegexpStemmer('s')\n",
    "ws.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f503052e-a055-41a2-8b27-93a4456d5942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.stem.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba532bcf-1a3b-4c51-9b30-8ce7bcb70a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hoal'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws1=nltk.stem.SnowballStemmer('spanish')\n",
    "ws1.stem('hoala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c61ca514-a3a9-4fb6-86df-26a27e800af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Srinath', ',', 'is', 'a', 'good', 'student']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import webtext\n",
    "sent_tokenizer=word_tokenize(\"Srinath, is a good student\")\n",
    "sent_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50bade9d-db4f-4c9a-b787-51ed1939540d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'srinath']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stop_words=set(stopwords.words('english'))\n",
    "words=\"Hello i am srinath\".split(\" \")\n",
    "l=[word for word in words if word not in eng_stop_words]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11a594c4-2a8a-461f-a6ad-a079693aa5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked\n",
      "When in full screen mode\n",
      "Pressing Ctrl-N should open a new browser when only download dialog is left open\n",
      "add icons to context menu\n",
      "So called \"tab bar\" should be made a proper toolbar or given the ability collapse / expand.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "text = webtext.raw()\n",
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sents_1 = sent_tokenizer.tokenize(text)\n",
    "print(sents_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ef21e14-e2c1-4603-9ad7-205cc3c42e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7744a8c0-5acc-4092-b386-2f4d6697d4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog.n.01'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "syn = wn.synsets('dog')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbca257f-d7a1-439b-a28d-4ee679315e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "['the dog barked all night']\n",
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(syn.definition())\n",
    "print(syn.examples())\n",
    "print(syn.hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01f3fb36-452c-4370-8363-68adeb9739b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bitch.n.04'),\n",
       " Synset('dog.n.01'),\n",
       " Synset('fox.n.01'),\n",
       " Synset('hyena.n.01'),\n",
       " Synset('jackal.n.01'),\n",
       " Synset('wild_dog.n.01'),\n",
       " Synset('wolf.n.01')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8de11e20-5aa1-4e00-b08a-d795e2fe8d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a78b661-eaa4-4b9e-9ade-025099eee364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "domestic_dog\n",
      "Canis_familiaris\n"
     ]
    }
   ],
   "source": [
    "lemmas = syn.lemmas()\n",
    "for i in lemmas:\n",
    "        print(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5efaee97-4792-4ead-a6b7-91ca538fe6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('evil.n.03.evil')]\n",
      "[Lemma('evil.n.03.evilness')]\n"
     ]
    }
   ],
   "source": [
    "syn1 = wn.synset('good.n.02')\n",
    "lemmas1 = syn1.lemmas()\n",
    "for i in lemmas1:\n",
    "    print(i.antonyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "093fae24-72ae-45f6-97dd-4e86fb6ddd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31a2a6b5-ebe4-410c-a7b2-ffd6fbdd16c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'runs', 'running', 'ran']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pattern\n",
    "from pattern.en import lexeme\n",
    "lexeme(\"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c34b7d-18d0-4e42-ab59-0ecfa6e12146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
